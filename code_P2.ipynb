{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335d1b21-407a-4fb1-bf00-76bc4eda8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch scikit-learn nltk tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1b9027-2833-4f94-8dd4-107515917c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.0 in /opt/conda/lib/python3.10/site-packages (3.8)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.0) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.0) (4.65.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the specific NLTK version\n",
    "!pip install nltk==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f14aad-a4f4-4be9-805d-9e6980d62a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa92dd07-f8d8-43f1-b1d9-a6339009f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f0261f-6e8a-4b30-a840-9a88a35f065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8df629-a44e-432a-92a4-ccdf47e1d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00922c6c-baa4-4e94-a1f3-5f115b134de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  toxicity  \\\n",
       "0   0  This is so cool. It's like, 'would you want yo...  0.000000   \n",
       "1   1  Thank you!! This would make my life a lot less...  0.000000   \n",
       "2   2  This is such an urgent design problem; kudos t...  0.000000   \n",
       "3   3  Is this something I'll be able to install on m...  0.000000   \n",
       "4   4               haha you guys are a bunch of losers.  0.893617   \n",
       "\n",
       "   severe_toxicity  obscene  threat   insult  identity_attack  sexual_explicit  \n",
       "0         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
       "1         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
       "2         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
       "3         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
       "4         0.021277      0.0     0.0  0.87234         0.021277              0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e55ba7-86b6-4342-a231-04b4544a6251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[ Integrity means that you pay your debts.]\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This is malfeasance by the Administrator and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@Rmiller101 - Spoken like a true elitist. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Paul: Thank you for your kind words.  I do, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sorry you missed high school. Eisenhower sent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0  [ Integrity means that you pay your debts.]\\n\\...\n",
       "1   1  This is malfeasance by the Administrator and t...\n",
       "2   2  @Rmiller101 - Spoken like a true elitist. But ...\n",
       "3   3  Paul: Thank you for your kind words.  I do, in...\n",
       "4   4  Sorry you missed high school. Eisenhower sent ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a602ce0-d983-44ce-ab9b-1b93b1369372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_data['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d5db056-6d50-448c-971d-f3d22afb9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Fill missing values in the 'text' column**\n",
    "train_data['text'] = train_data['text'].fillna('')\n",
    "test_data['text'] = test_data['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fe3d966-f360-427d-acff-2c94f6d42888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target labels\n",
    "target_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ef5eae4-5677-4ebc-a09d-203965e2eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    tokens = word_tokenize(text, preserve_line=True)  # Tokenize without sentence segmentation\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "train_data['tokens'] = train_data['text'].apply(preprocess_text)\n",
    "test_data['tokens'] = test_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5df492e3-c13a-4e8f-8bdd-009dd8baed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "all_tokens = [token for tokens in train_data['tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens)\n",
    "vocab = {token: idx+1 for idx, (token, _) in enumerate(token_counts.items())}  # +1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6ca2649-498a-4970-b740-10fe89c7819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to sequences of indices\n",
    "def tokens_to_indices(tokens):\n",
    "    return [vocab.get(token, 0) for token in tokens]  # 0 for unknown words\n",
    "\n",
    "train_data['sequence'] = train_data['tokens'].apply(tokens_to_indices)\n",
    "test_data['sequence'] = test_data['tokens'].apply(tokens_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b149224c-6c8d-4caa-bd88-0371e58fd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "def pad_sequence(sequence, max_len=400):  \n",
    "    if len(sequence) > max_len:\n",
    "        return sequence[:max_len]\n",
    "    return sequence + [0] * (max_len - len(sequence))\n",
    "\n",
    "train_data['sequence'] = train_data['sequence'].apply(lambda x: pad_sequence(x, max_len=400))\n",
    "test_data['sequence'] = test_data['sequence'].apply(lambda x: pad_sequence(x, max_len=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "173f3eda-273d-4722-b65b-4f9d1753b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset class\n",
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.sequences[idx], dtype=torch.long),\n",
    "            torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300de5d4-9b98-4876-82af-c8e9f77734dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data['sequence'].tolist(),\n",
    "    train_data[target_labels].values,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = ToxicCommentDataset(X_train, y_train)\n",
    "val_dataset = ToxicCommentDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Reduced batch size for better gradient updates\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c428645-ebfc-4876-805b-4f7dcc318dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_GRU_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, lstm_hidden_dim=128, gru_hidden_dim=128, cnn_out_channels=128, output_dim=7, dropout_rate=0.5):\n",
    "        super(BiLSTM_GRU_Model, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(embedding_dim, lstm_hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(lstm_hidden_dim * 2, gru_hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Dropout after GRU\n",
    "        self.dropout_gru = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv1d(in_channels=gru_hidden_dim * 2, out_channels=cnn_out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Dropout after CNN\n",
    "        self.dropout_cnn = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(cnn_out_channels * 400, output_dim)  # Adjusted for increased max_len\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embedded)\n",
    "        gru_out, _ = self.gru(lstm_out)\n",
    "        gru_out = self.dropout_gru(gru_out)\n",
    "        \n",
    "        # Change the shape for convolutional layer\n",
    "        gru_out = gru_out.permute(0, 2, 1)\n",
    "        conv_out = self.conv(gru_out)\n",
    "        conv_out = self.dropout_cnn(conv_out)\n",
    "        \n",
    "        # Flatten\n",
    "        conv_out = conv_out.view(conv_out.size(0), -1)\n",
    "        out = self.fc(conv_out)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23c9be37-c334-469c-bd6e-413b1f1301e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced dataset\n",
    "class_counts = train_data[target_labels].sum(axis=0).values\n",
    "class_weights = 1.0 / class_counts\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BiLSTM_GRU_Model(len(vocab) + 1).to(device)\n",
    "criterion = nn.BCELoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)  # Reduced learning rate for finer updates\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)  # More frequent learning rate reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "536260b8-74ed-4657-b39b-7cc9452bb856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101525/101525 [35:59<00:00, 47.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.0000, Validation Loss: 0.0000, Validation Accuracy: 0.9853, Validation Hamming Loss: 0.0147, Validation F1 Score: 0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101525/101525 [36:05<00:00, 46.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Train Loss: 0.0000, Validation Loss: 0.0000, Validation Accuracy: 0.9900, Validation Hamming Loss: 0.0100, Validation F1 Score: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101525/101525 [35:55<00:00, 47.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Train Loss: 0.0000, Validation Loss: 0.0000, Validation Accuracy: 0.9872, Validation Hamming Loss: 0.0128, Validation F1 Score: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3  # Increased epochs for more training iterations\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for sequences, targets in tqdm(train_loader):\n",
    "        sequences, targets = sequences.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    val_preds = np.concatenate(val_preds, axis=0)\n",
    "    val_targets = np.concatenate(val_targets, axis=0)\n",
    "\n",
    "    # Binarize predictions and targets\n",
    "    val_preds = (val_preds >= 0.5).astype(int)\n",
    "    val_targets = val_targets.astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "    val_f1_score = f1_score(val_targets, val_preds, average='micro')\n",
    "    val_hamming_loss = hamming_loss(val_targets, val_preds)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "        f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "        f\"Validation Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "        f\"Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "        f\"Validation Hamming Loss: {val_hamming_loss:.4f}, \"\n",
    "        f\"Validation F1 Score: {val_f1_score:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# Load the best model for predictions\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3607988-d213-45e3-8970-d44bc19dfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "test_sequences = torch.tensor(test_data['sequence'].tolist(), dtype=torch.long).to(device)\n",
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_sequences), 16):  # Reduced batch size for better memory usage during inference\n",
    "        batch = test_sequences[i:i + 16]\n",
    "        outputs = model(batch)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Binarize predictions\n",
    "predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "# Create submission DataFrame\n",
    "pred_df = pd.DataFrame(predictions, columns=target_labels)\n",
    "\n",
    "# Include 'id' column if present in test data\n",
    "if 'id' in test_data.columns:\n",
    "    pred_df.insert(0, 'id', test_data['id'].values)\n",
    "\n",
    "# Save predictions to CSV\n",
    "pred_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
